{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-glAZRvz_yBK"
      },
      "outputs": [],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key = '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')"
      ],
      "metadata": {
        "id": "K9jEowBv_zri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "UoDjoFVHDGZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(question, essay, max_response_length=300):\n",
        "\n",
        "    prompt_tokens = len((f\"Evaluate the Task Response, Coherence and Cohesion, Lexical Resource, Grammatical Range and Accuracy of the essay in {max_response_length} words. Question: {question}. Essay: {essay}\").split())\n",
        "    prompt_token_estimate = prompt_tokens * 4\n",
        "    max_tokens_allowed = 4097 - prompt_token_estimate\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "      model=\"gpt-3.5-turbo-1106\",\n",
        "      messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a IELTS examinator.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Evaluate the Task Response, Coherence and Cohesion, Lexical Resource, Grammatical Range and Accuracy of the essay in 300 words. Question: {question}. Essay: {essay}\"}\n",
        "      ],\n",
        "      max_tokens=max_tokens_allowed\n",
        "    )\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "cjGjmvGl_1lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/ielts.csv')\n",
        "data = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "1bTXmM7FAGHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[['Task_Type', 'Question', 'Essay', 'Overall']]\n",
        "data['Summary'] = np.nan"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEy1gHfMDDCp",
        "outputId": "31d97102-9edd-4dec-9895-150c87ea9a44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-42-011b1d7604df>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Summary'] = np.nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-a7rY2EdPyO4",
        "outputId": "7d38bb65-9da8-4b7c-a007-d8fa4cda3e5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1435 entries, 0 to 1434\n",
            "Data columns (total 5 columns):\n",
            " #   Column     Non-Null Count  Dtype  \n",
            "---  ------     --------------  -----  \n",
            " 0   Task_Type  1435 non-null   int64  \n",
            " 1   Question   1435 non-null   object \n",
            " 2   Essay      1435 non-null   object \n",
            " 3   Overall    1435 non-null   float64\n",
            " 4   Summary    1435 non-null   object \n",
            "dtypes: float64(1), int64(1), object(3)\n",
            "memory usage: 56.2+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for index in tqdm(data.index, desc=\"Generating Summaries\"):\n",
        "  if pd.isna(data.at[index, 'Summary']):\n",
        "    summary = evaluate(data.at[index, 'Question'], data.at[index, 'Essay'])\n",
        "    data.at[index, 'Summary'] = summary.choices[0].message.content\n",
        "    data.to_csv('/content/drive/MyDrive/ielts.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikmfgFBlKOE4",
        "outputId": "c9be1586-d928-4441-8668-718e9c5fc46c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Summaries: 100%|██████████| 1435/1435 [4:13:44<00:00, 10.61s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_csv('ielts.csv', index=False)"
      ],
      "metadata": {
        "id": "-yvJI2HDOKYu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}